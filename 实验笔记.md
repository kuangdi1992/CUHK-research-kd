## 2018年12月20日 ##

### atom使用方法 ###
１．使用快捷键 Ctrl + Shift + M 则可打开Markdown的预览界面.<br>
２．常用快捷键：<br>
Crtl+Shift+M    开启Markdown实时预览<br>
Command+Shift+P    打开命令窗口，可以运行各种菜单功能<br>
Command + T    快速多文件切换<br>
Command + F    文件内查找和替换<br>
Command + Shift + F    多文件查找和替换<br>
Command + [    对选中内容向左缩进<br>
Command + ]    对选中内容向右缩进<br>
Command + \    显示或隐藏目录树<br>
Crtl + m    相应括号之间，html tag之间等跳转<br>
Crtl + Alt + B    格式化代码（需要安装atom-beautify）<br>

### md文件的语法 ###
1.网址：https://www.jianshu.com/p/f8021c881d0f

２.代码块：<pre><code>tell application "Foo"
    beep
end tell
</code></pre>

### pytorch中tensor和numpy之间相互转换的方法 ###
１．加载需要用到的模块<br>
import torch<br>
from torch.autograd import Variable<br>
import matplotlib.pyplot as plt<br>
import matplotlib.image as mpimg<br>

2.将Tensor张量转化为numpy矩阵<br>
sub_np1 = sub_ts.numpy() <br>

3.将numpy矩阵转换为Tensor张量<br>
sub_ts = torch.from_numpy(sub_img) <br>

4.将numpy转换为Variable<br>
sub_va = Variable(torch.from_numpy(sub_img))<br>

5.将Variable张量转化为numpy<br>
sub_np2 = sub_va.data.numpy()

### 用numpy转换后进行阈值量化的代码 ###
<pre><code>
def gradient_execute(net):
  standard = 32　这里的３２值得考虑下？
  paralist = []
  for param in net.parameters():
      temp = copy.deepcopy(param.grad.data)
      temp1 = temp.cpu().numpy()
      num = temp1.size
      duan = 2
      gradient_min=np.min(temp1)
      gradient_max=np.max(temp1)
      maxf1 = abs(gradient_max - gradient_min)
      maxf2 = gradient_min
      duan_t = 1.0/duan
      jiange = maxf1*duan_t
      pnum = []
      pnum.append(temp1[temp1 < (maxf2 + jiange)].size)
      for i in range(2, duan):
          pnum.append(temp1[(temp1 < (maxf2 + i*jiange))&(temp1 >= (maxf2 + (i-1)jiange))].size)
      pnum.append(temp1[temp1 >= (maxf2 + (duan-1)jiange)].size)
      ppnum = []
      for pnum1 in pnum:
          ppnum.append(pnum1 / sum(pnum))
      hx = 0
      for i in range(duan):
          if ppnum[i] == 0:
              hi = 0
          else:
              hi = -ppnum[i] * np.log2(ppnum[i])
          hx = hx + hi
      k = int(num - num * (hx / standard))
      temp2 =temp1.flatten()
      temp2 = np.sort(temp2)
      threshold = float(temp2[k])
      temp[abs(temp) >= threshold] = 0
      param.grad.data[abs(param.grad.data) < threshold] = 0
      paralist.append(temp)
  return paralist
</code></pre>

## 2018年12月21日 ##

１．问题一：<br>
<pre><code>
Traceback (most recent call last):
  File "/home/kd/PycharmProjects/cifar10/Dis_CIFAR10_threshold_entropy.py", line 313, in <module>
    run()
  File "/home/kd/PycharmProjects/cifar10/Dis_CIFAR10_threshold_entropy.py", line 247, in run
    loss.backward()
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 341, in reduction_fn_nccl
    group=self.nccl_reduction_group_id)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/distributed/__init__.py", line 306, in all_reduce_multigpu
    return torch.C.dist_all_reduce_multigpu(tensor_list, op, group)
RuntimeError: NCCL error in: /pytorch/torch/lib/THD/base/data_channels/DataChannelNccl.cpp:322, unhandled system error
</code></pre>

百度翻译出来的解释是未处理的系统错误，现在还不是很清楚是由什么引起的，但是换了一段代码之后是可以跑的，只是速度很慢。<br>

**问题２**：
<pre><code>
Timestamp: 2018-12-21 19:20:21.724336 | Iteration:      0 | Loss: 2.3010 | Accuracy: 0.1094 | Total_param: 2472266 | Sparse_param: 566358 | Mini_Batch_Time: 2.1386 |
Timestamp: 2018-12-21 19:20:43.831311 | Iteration:     20 | Loss: 2.3026 | Accuracy: 0.0938 | Total_param: 2472266 | Sparse_param: 544751 | Mini_Batch_Time: 1.1042 |
Timestamp: 2018-12-21 19:21:05.215620 | Iteration:     40 | Loss: 2.2999 | Accuracy: 0.1406 | Total_param: 2472266 | Sparse_param: 595424 | Mini_Batch_Time: 1.1209 |
Timestamp: 2018-12-21 19:21:26.700141 | Iteration:     60 | Loss: 2.3034 | Accuracy: 0.1875 | Total_param: 2472266 | Sparse_param: 607946 | Mini_Batch_Time: 1.1506 |
Timestamp: 2018-12-21 19:21:48.395298 | Iteration:     80 | Loss: 2.3008 | Accuracy: 0.1562 | Total_param: 2472266 | Sparse_param: 510777 | Mini_Batch_Time: 1.1395 |
Timestamp: 2018-12-21 19:22:10.364666 | Iteration:    100 | Loss: 2.3019 | Accuracy: 0.0781 | Total_param: 2472266 | Sparse_param: 538194 | Mini_Batch_Time: 1.1063 |
Timestamp: 2018-12-21 19:22:32.098194 | Iteration:    120 | Loss: 2.3033 | Accuracy: 0.1406 | Total_param: 2472266 | Sparse_param: 413330 | Mini_Batch_Time: 1.0467 |
Timestamp: 2018-12-21 19:22:54.052004 | Iteration:    140 | Loss: 2.3021 | Accuracy: 0.0625 | Total_param: 2472266 | Sparse_param: 395402 | Mini_Batch_Time: 0.9916 |
Timestamp: 2018-12-21 19:23:14.845561 | Iteration:    160 | Loss: 2.3012 | Accuracy: 0.0938 | Total_param: 2472266 | Sparse_param: 554283 | Mini_Batch_Time: 1.0807 |
Timestamp: 2018-12-21 19:23:36.489221 | Iteration:    180 | Loss: 2.3000 | Accuracy: 0.1406 | Total_param: 2472266 | Sparse_param: 555110 | Mini_Batch_Time: 0.9995 |
Timestamp: 2018-12-21 19:23:58.206874 | Iteration:    200 | Loss: 2.3016 | Accuracy: 0.1094 | Total_param: 2472266 | Sparse_param: 568844 | Mini_Batch_Time: 1.0297 |
Timestamp: 2018-12-21 19:24:20.185524 | Iteration:    220 | Loss: 2.3009 | Accuracy: 0.1406 | Total_param: 2472266 | Sparse_param: 436651 | Mini_Batch_Time: 1.0444 |
Timestamp: 2018-12-21 19:24:42.198502 | Iteration:    240 | Loss: 2.3025 | Accuracy: 0.0469 | Total_param: 2472266 | Sparse_param: 446702 | Mini_Batch_Time: 1.0674 |
Timestamp: 2018-12-21 19:25:03.795029 | Iteration:    260 | Loss: 2.3036 | Accuracy: 0.1406 | Total_param: 2472266 | Sparse_param: 304545 | Mini_Batch_Time: 0.9872 |
Timestamp: 2018-12-21 19:25:24.621977 | Iteration:    280 | Loss: 2.2946 | Accuracy: 0.1875 | Total_param: 2472266 | Sparse_param: 638930 | Mini_Batch_Time: 0.7105 |
Timestamp: 2018-12-21 19:25:38.930669 | Iteration:    300 | Loss: 2.3043 | Accuracy: 0.1094 | Total_param: 2472266 | Sparse_param: 395204 | Mini_Batch_Time: 0.7233 |
Timestamp: 2018-12-21 19:25:50.871713 | Iteration:    320 | Loss: 2.3015 | Accuracy: 0.0625 | Total_param: 2472266 | Sparse_param: 357183 | Mini_Batch_Time: 0.5297 |
Timestamp: 2018-12-21 19:26:03.117471 | Iteration:    340 | Loss: 2.3013 | Accuracy: 0.1562 | Total_param: 2472266 | Sparse_param: 306636 | Mini_Batch_Time: 0.5176 |
Timestamp: 2018-12-21 19:26:15.108734 | Iteration:    360 | Loss: 2.3008 | Accuracy: 0.0781 | Total_param: 2472266 | Sparse_param: 541934 | Mini_Batch_Time: 0.7169 |
Timestamp: 2018-12-21 19:26:26.613563 | Iteration:    380 | Loss: 2.3015 | Accuracy: 0.0625 | Total_param: 2472266 | Sparse_param: 353753 | Mini_Batch_Time: 0.5169 |
</code></pre>
从上面的结果中可以看出我们的算法还是有效果的。<br>
现在的问题是：怎样将发送的梯度的数量进一步的变小？<br>
目前想到的解决方法是：将standard的值增大<br>  

**问题３**
节点之间进行传输的代码如下：
<pre><code>
def trans_gradients(net):
    world_size = float(dist.get_world_size())
    values = []
    global index
    global Total_param_num
    global Sparse_param_num
    Total_param_num = 0
    Sparse_param_num = 0
    for i, param in enumerate(net.parameters()):
        temp_param = copy.deepcopy(param.grad.data).cpu()
        Total_param_num += param.grad.data.nelement()
        values.append(temp_param[temp_param != 0])
        if len(values[i]) != 0:
            index = torch.FloatTensor(np.where(temp_param.cpu() != 0))
        else:
            index = torch.FloatTensor([])

        global index_list
        global value_list
        global size_list

        # Get index and values size
        # selfsize
        size = torch.IntTensor([len(index), len(index[0]) if len(index) != 0 else 0]).cuda()
        # initial size_list for gather whole group tensor of size
        size_list = [torch.IntTensor([2, 2]).cuda() for j in range(args.world_size)]
        dist.all_gather(size_list, size)
        max_index0 = int(max(size_list[j][0] for j in range(args.world_size)))
        max_index1 = int(max(size_list[j][1] for j in range(args.world_size)))

        if max_index0==0 :
            continue
        else :
            # Get index of sparse matrix
            # selfindex and scalar index
            supplement_index = torch.FloatTensor([-1 for j in range(max_index1 - (len(index[0]) if len(index) != 0 else 0))])
            temp_index = torch.FloatTensor(max_index0, max_index1)
            if len(index) == 0:
                for j in range(max_index0):
                    temp_index[j] = supplement_index
            else:
                for j in range(max_index0):
                    temp_index[j] = torch.cat((index[j], supplement_index), 0)

            index = temp_index
            Sparse_param_num += (index.nelement() + max_index1)


            # initial index_list for gather whole group tensor of index
            index_list = [torch.FloatTensor([[2 for k in range(max_index1)] for j in range(max_index0)]) for l in range(args.world_size)]
            # dist.all_gather(index_list, index)


            # Get values of sparse matrix
            # selfvalues
            # initial supplement_value for values[i]
            supplement_value = torch.FloatTensor([-1 for j in range(max_index1 - len(values[i]))])
            temp_value = torch.cat((values[i], supplement_value), 0)


            # initial value_list for gather whole group tensor of value
            value_list = [torch.FloatTensor([2 for k in range(max_index1)]) for j in range(args.world_size)]
            # dist.all_gather(value_list, torch.cat((values[i], supplement_value), 0).cuda())


            supplement_indexval = []
            for j in range(args.world_size) :
                supplement_indexval.append( torch.cat( (index_list[j].view(1,-1)[0],value_list[j]) , 0 ).view(-1,max_index1).numpy() )


            # initial indexval_list for gather whole group tensor of indexvar
            indexval_list = [ torch.HalfTensor( supplement_indexval[j] ).cuda() for j in range(args.world_size) ]
            dist.all_gather( indexval_list, torch.HalfTensor( torch.cat( (index.view(1,-1)[0],temp_value) , 0 ).view(-1,max_index1).numpy() ).cuda() )


            # Wash the data of index and values
            for j in range(args.world_size):
                index_list[j] = torch.FloatTensor(indexval_list[j][:len(indexval_list[j])-1].cpu().numpy())
                value_list[j] = torch.FloatTensor(indexval_list[j][len(indexval_list[j])-1:].cpu().numpy())
                index_list[j] = index_list[j][index_list[j] != -1]
                value_list[j] = value_list[j][value_list[j] != -1]
                if len(index_list[j]) != 0:
                    index_list[j] = index_list[j].view(size_list[j][0], size_list[j][1])
                    param.grad.data += torch.sparse.FloatTensor(torch.LongTensor(index_list[j].numpy()), value_list[j],param.size()).to_dense().cuda()
            param.grad.data /= world_size
</code></pre>

**问题４**：又出现了一个爆内存的问题：
<pre><code>
Traceback (most recent call last):
  File "/home/kd/PycharmProjects/cifar10/AlexNet_Var.py", line 295, in <module>
    run()
  File "/home/kd/PycharmProjects/cifar10/AlexNet_Var.py", line 258, in run
    logs[-1]['test_loss'], logs[-1]['test_accuracy'] = evaluate(net, test_loader, )
  File "/home/kd/PycharmProjects/cifar10/AlexNet_Var.py", line 284, in evaluate
    output = net(inputs)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(input, kwargs)
  File "/home/kd/PycharmProjects/cifar10/AlexNet_Var.py", line 50, in forward
    x = self.features(x)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(input, kwargs)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(input, kwargs)
  File "/home/kd/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA error: out of memory
</code></pre>
问题应该是出在logs[-1]['test_loss'], logs[-1]['test_accuracy'] = evaluate(net, test_loader, )这一行里面？<br>
问题已经找出来了，主要是test中的batchsize设置的太大了，一次性读入的参数过多，多于了显存，所以报错了。<br>

---------------------------------------------------------------------------------------------
## 2018年12月23日 ##

### 服务器上运行py文件的代码 ###
<pre><code>
python AlexNet_kd.py --dist-rank=1 --dir-data='/home/yeyongjie/kuangdi/data'   
</code></pre>

### 今天遇到的问题 ###
问题一：<br>
在进行训练时，运行到第９个或者第１０个epoch的时候，会出现下面的错误：<br>
<pre><code>
Traceback (most recent call last):
  File "AlexNet_kd.py", line 284, in <module>
    run()
  File "AlexNet_kd.py", line 220, in run
    paralist = gradient_execute(net)
  File "AlexNet_kd.py", line 181, in gradient_execute
    trans_gradients(net)
  File "AlexNet_kd.py", line 149, in trans_gradients
    param.grad.data += torch.sparse.FloatTensor(torch.LongTensor(index_list[j].numpy()), value_list[j],param.size()).to_dense().cuda()
RuntimeError: indices and values must have same nnz
</code></pre>
解决方法：
<pre><code>
if len(index_list[j]) != 0:
  index_list[j] = index_list[j].view(size_list[j][0], size_list[j][1])
  if len(index_list[j][0]) == len(value_list[j]):
    param.grad.data += torch.sparse.FloatTensor(torch.LongTensor(index_list[j].numpy()), value_list[j],param.size()).to_dense().cuda()
</code></pre>

问题二：<br>
在修改了standard之后，压缩率确实是增加了，但是准确率却没有之前那么高了，所以还是要想办法来将准确率调节好。

----------------------------------------------------------------------------------------------------------
## 2019年1月8日 ##
问题一：</br>
发现了一个网址，主要是讲自动调优超参数的，网址是：http://www.myzaker.com/article/595741711bc8e0fa2900000b/

## 2019年1月9日 ##
问题一：在对LSTM-PTB进行自己的算法运行的时候，出现了一个尺寸大小的问题，如下。</br>
<pre><code>
Traceback (most recent call last):
  File "/home/kd/PycharmProjects/LSTM-PTB-kd/main-kd.py", line 391, in <module>
    train()
  File "/home/kd/PycharmProjects/LSTM-PTB-kd/main-kd.py", line 335, in train
    paralist = gradient_execute(model)
  File "/home/kd/PycharmProjects/LSTM-PTB-kd/main-kd.py", line 290, in gradient_execute
    trans_gradients(net)
  File "/home/kd/PycharmProjects/LSTM-PTB-kd/main-kd.py", line 255, in trans_gradients
    param.grad.data += torch.sparse.FloatTensor(torch.LongTensor(index_list[j].numpy()), value_list[j],param.size()).to_dense().cuda()
RuntimeError: sizes is inconsistent with indices: for dim 0, size is 10000 but found index 10000
</code></pre>
这个问题没有解决，也不想解决了，主要是因为在后面进行画图的时候，只需要epoch和其他的数据，并不需要时间，所以可以不需要这个方法来减少时间，直接将全部的梯度发送过去就可以了。<br>
**问题二：**
对于将训练和测试的数据写到csv文件中的代码，应该是如下:<br>
<pre><code>
def train():
    # Turn on training mode which enables dropout.
    logs_epoch = []
    model.train()
    total_loss = 0
    start_time = time.time()
    ntokens = len(corpus.dictionary)
    hidden = model.init_hidden(args.batch_size)
    for i,data in enumerate(train_data,0):
        datai, targets = data
        # Starting each batch, we detach the hidden state from how it was previously produced.
        # If we didn't, the model would try backpropagating all the way to start of the dataset.
        for batch_num in range(datai.size(0)):
            hidden = repackage_hidden(hidden)
            model.zero_grad()
            output, hidden = model(datai[batch_num], hidden)
            loss = criterion(output.view(-1, ntokens), targets[batch_num])
            loss.backward()
            average_gradients(model)
            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
            for p in model.parameters():
                p.data.add_(-lr, p.grad.data)

            total_loss += loss.data
        cur_loss = total_loss[0] / args.batch_size
        elapsed = time.time() - start_time
        log_obj = {
            'timestamp':datetime.now(),
            'iteration':i,
            'loss':cur_loss,
            'total_param':Total_param_num,
            'ppl':math.exp(cur_loss),
            'mini_batch_time':elapsed * 1000 / args.batch_size
        }
        total_loss = 0
        start_time = time.time()
        print("Timestamp: {timestamp} | "
                  "Iteration: {iteration:6} | "
                  "Loss: {loss:6.4f} | "
                  "Total_param: {total_param:6} | "
                  "ppl: {ppl:8.2f} | "
                  "Mini_Batch_Time: {mini_batch_time:6.4f} | ".format(/*/*log_obj))
        logs_epoch.append(log_obj)

        # print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | Total {:10.2f}'
        #             'loss {:5.2f} | ppl {:8.2f}'.format(
        #         epoch, i, len(train_data), lr,
        #         elapsed * 1000 / len(train_data), Total_param_num, cur_loss, math.exp(cur_loss)))

    return logs_epoch

# Loop over epochs.
lr = args.lr
best_val_loss = None

# At any point you can hit Ctrl + C to break out of training early.
try:
    dist.init_process_group(backend='nccl', init_method=args.init_method, rank=args.dist_rank,
                            world_size=args.world_size)
    logs = []
    for epoch in range(1, args.epochs+1):
        epoch_start_time = time.time()
        print("epoch {:3d}".format(epoch))
        logs.append(train())
        val_loss = evaluate(val_data)
        logs[epoch - 1][-1]['test_loss'] = val_loss
        logs[epoch - 1][-1]['test_ppl'] = math.exp(val_loss)
        print("Timestamp: {timestamp} | "
              "Iteration: {iteration:6} | "
              "Loss: {loss:6.4f} | "
              "Total_param: {total_param:6} | "
              "ppl: {ppl:8.2f} | "
              "Mini_Batch_Time: {mini_batch_time:6.4f} | "
              "Test Loss: {test_loss:5.2f} | "
              "Test ppl: {test_ppl:8.2f}".format(**logs[epoch - 1][-1]))
        df = pd.DataFrame(logs[epoch - 1])
        # print (df)
        df.to_csv('./log/{}_Node{}_{}.csv'.format(args.file_name, args.dist_rank,
                                                  datetime.now().strftime("%Y-%m-%d %H:%M:%S")), index_label='index')
        # print('-' * 89)
        # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
        #         'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
        #                                    val_loss, math.exp(val_loss)))
        # print('-' * 89)
        # Save the model if the validation loss is the best we've seen so far.
        if not best_val_loss or val_loss < best_val_loss:
            with open(args.save, 'wb') as f:
                torch.save(model, f)
            best_val_loss = val_loss
        else:
            # Anneal the learning rate if no improvement has been seen in the validation dataset.
            lr /= 4.0
</code></pre>
其中还有一个问题，对于loss，写入csv中的是tensor，所以现在想的解决方法是<pre><code>'loss':cur_loss.cpu().numpy()</code></pre>

那么现在对于PTB上的所有的问题基本上都解决了。
## 2019年1月12日 ##
现状：在天河上安装了conda虚环境kd，但是怎么安装torchaudio还是没有找到方法，今天身体有点不舒服，所以还是好好休息下，晚上再来。加油
## 2019年1月17日 ##
新的方法：
<pre><code>
temp = copy.deepcopy(param.grad.data)
temp1 = temp.cpu().numpy()
duan = 5
gradient_min = np.min(temp1)
gradient_max = np.max(temp1)
maxf1 = abs(gradient_max - gradient_min)
maxf2 = gradient_min
duan_t = 1.0 / duan
jiange = maxf1 * duan_t
pnum = []
pnum.append(temp1[temp1 < (maxf2 + jiange)].size)
for i in range(2, duan):
    pnum.append(temp1[(temp1 < (maxf2 + i * jiange)) & (temp1 >= (maxf2 + (i - 1)*jiange))].size)
pnum.append(temp1[temp1 >= (maxf2 + (duan - 1)*jiange)].size)
if np.min(pnum) == 0:
    ix = np.argsort(pnum)[1]
else:
    ix = np.argmin(pnum)
if ix == 0:
    threshold = maxf2 + jiange
    temp[temp < threshold] = 0
    param.grad.data[param.grad.data >= threshold] = 0
elif ix == duan - 1:
    threshold = maxf2 + (duan - 1)*jiange
    temp[temp >= threshold] = 0
    param.grad.data[param.grad.data < threshold] = 0
else:
    threshold1 = maxf2 + (ix+1) * jiange
    threshold2 = maxf2 + ix * jiange
    temp[((temp >= threshold2) & (temp < threshold1))] = 0
    param.grad.data[((param.grad.data < threshold2) | (param.grad.data >= threshold1))] = 0
paralist.append(temp)
效果并不是很好，所以应该是会丢弃掉这个方法
</code></pre>
## 2019年1月27日 ##
今天的进展主要有以下几点：<br>
1.在天河上跑了两个节点的PTB结果自己的方法的效果是最好的<br>
2.跑了下ResNet18的代码，中途出了问题，可能是天河自己关了连接的原因，明天继续好好弄下<br>
3.英文论文的摘要写了一半了<br>
明天的任务：<br>
1.将除了AN4的其他所有的模型的代码（包括Baseline，Aji和自己的方法）以及参数全部弄好(完成)<br>
2.将英文论文的abstract弄好（中文的）（完成）<br>
3.将introduction弄1/4.<br>
4.将自适应的那篇论文的摘要弄出来做好记录，这个应该是可以有所借鉴的，加油（完成）<br>
5.每天的任务好好完成，自己要对自己有点要求<br>

将整体的论文框架弄出来了，后面两天专心跑实验，加上专心写论文。

另外，在我的工作中可以将standard设置为不同的值的效果写出来，包括一个表和一个图。

## 2019年1月29日 ##
今天是回家前的最后一天了，今天的任务很重，必须把实验前的所有准备都弄好了，不然回家不好
