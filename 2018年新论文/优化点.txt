
从PS更新参数本身：
YY	1.将聚合参数做平均的方式改变：想法是加入随机性（例如遗传算法），或者参考过拟合中正则项的做法，从空间维度对不同worker得到的参数做加权等
ZM/HN	2.保证参数的一致性：更新机制采用并发控制的方式，起到同步的效果
JYF	3.对训练方式采用自适应策略：例如前面异步，后面同步
CMQ	4.对梯度进行评估：在训练过程中一边计算，一边评估，进行舍弃等操作（参考李沐参数服务器的过滤方式）
KD	5.改变主动性：ps可以强制对worker进行push参数
ps部署方式+异构的任务分配方式：
DX	1.ps如何部署：包括ps的个数和放置
2.有加速卡时如何分配任务
异构优化：
DX	1.节点内同步和节点间异步：节点内CPU作为ps，GPU作为worker，进行同步训练，节点间异步，每隔一段时间强制做一次同步
LXX	2.计算任务分配：例如针对MIC+CPU分配不同的batch大小，做到尽量同步
LXX	3.层次化聚合梯度，利用共享内存，关注节点内的聚合方式，将不同worker的参数存为同一份